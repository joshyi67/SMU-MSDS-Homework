---
title: "Doing Data Science Case Study 2"
author: "Joshua Yi & Grant Bourzikas"
date: "November 24, 2018"
output: 
  html_document:
    df_print: paged
---
  
```{r loadlib, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(kableExtra)
library(ggplot2)
library(fpp2) 
library(tibble)
library(dplyr)
library(fastDummies)
library(GGally)
library(glmnet)
library(MASS)
library(caret)
library(pROC)
library(ROCR)
library(pheatmap)
library(randomForest)
library(mlbench)
library(caret)
library(class)
library(FNN)
library(stringr)
library(reshape)
library(kknn)
library(dplyr)
library(kableExtra)
library(formattable)
library(gtable)
library(grid)
library(gridExtra)
library(cowplot)
library(ggpubr)
```

### **Introduction**
Yi & Bourzikas specializes in talent management solutions for Fortune 1000 companies focus on building and developing strategies for retaining employees. We specialize in workforce planning, employee training programs, identifying high-potential employees and reducing/preventing voluntary employee turnover (attrition). As part of this engagement, our data science team will predict for your organization. 
```{r ReadDataIn}
employee <- read.csv("CaseStudy2-data.csv", na.strings = "NULL")
employeeValidation <- read.csv("CaseStudy2Validation.csv", na.strings = "NULL")
result <-rbind(employee,employeeValidation)
```

```{r CreateDummy}
#Create 1/0 from Catagorical Variables
emp_train <- fastDummies::dummy_cols(employee) # Create Dummy Variables
emp_test <- fastDummies::dummy_cols(employeeValidation) # Create Dummy Variables
emp_result <- rbind(emp_test, emp_test) # combine train and test data sets
```

```{r CreateVariables}
# Creating Variables
# Define Data Colums to Make it Easier
cols.Base <- c(2:36)
cols.CatAttr <- c(38:39)
cols.CatAll <- c(40:68)
col.NoJobRole <- c(1,2,5,7,8,10,12,14,15,18,20,21,22,25:36,40:42,52:53,63:68)
names(emp_result[,c(col.NoJobRole)])
# Removed 17 From Data Set
cols.RemoveJobRoleCat <- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,18,19,20,21,22,66,24,25,26,27,28,29,30,31,32,33,34,35,36)
# All Job Detailed Roles
cols.JobRoles <- c(54:62)
cols.AllButAttr <- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,24,25,26,27,28,29,30,31,32,33,34,35,36,40,41,42,43,44,45,46,47,48,49,50,51,52,53,63,64,65,66,67,68)
# This is all the Catagorical Fields
cols.CatGLM <- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,66,24,25,26,27,28,29,30,31,32,33,34,35,36)
cols.CatKNN <- c(1,2,3,5,7,8,10,11,12,14,15,16,18,20,21,22,25,26,27,28,29,30,31,32,33,34,35,36,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68)
cols.NumericAll <- c(1,2,5,7,8,10,11,12,14,15,16,18,20,21,22,25,26,27,28,29,30,31,32,33,34,35,36,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68)
cols.Attrition <- 34
cols.KeyFieldsBaseModel <- c(40:42,7,12,63:65,22,67:68,27,30,31:36)
```

We received the data files from your employee database and have outlined some key highlights.  The following charts are part of our exploratory data and will give your organization an idea of how different features in the data set apply.  The pair plots show all the variables based on whether your employees have left the organisation.

```{r EDA}
# Basic EDA
#EDA - Exploratory Not for Report
pairs(emp_result[,c(2:5)], col=emp_train$Attrition)
pairs(emp_result[,c(6:10)], col=emp_train$Attrition)
pairs(emp_result[,c(11:15)], col=emp_train$Attrition)
pairs(emp_result[,c(16:20)], col=emp_train$Attrition)
pairs(emp_result[,c(21:25)], col=emp_train$Attrition)
pairs(emp_result[,c(26:30)], col=emp_train$Attrition)
pairs(emp_result[,c(31:35)], col=emp_train$Attrition)
pairs(emp_result[,c(36:40)], col=emp_train$Attrition)
```

####  *Heat Map Charts*

Because of the data that we were able to analyze as part of the Par Plots above, we developed 2 Heat Maps and Correlations and Distribution Matrix to take a deeper dive in the data set.

```{r HeatMap}
# Heat Map for All Fields
employeeHeatMap <- round(cor(emp_result[,c(cols.NumericAll)]),2)
melted_employeeHeatMap <- melt(employeeHeatMap)
ggplot(data = melted_employeeHeatMap, aes(x=X1, y=X2, fill=value)) + 
  theme(axis.text.x  = element_blank(),axis.ticks.x=element_blank(),axis.title.x=element_blank(),axis.text.y  = element_text(size = 7))+geom_tile()
#ggsave("images/employeeHeatMap.png",plot = last_plot(), type = png())
# Heat Map for Key Sign Fields
employeeHeatMapSig <- round(cor(emp_result[,c(cols.KeyFieldsBaseModel)]),2)
melted_employeeHeatMapSig <- melt(employeeHeatMapSig)
ggplot(data = melted_employeeHeatMapSig, aes(x=X1, y=X2, fill=value)) + 
  theme(axis.text.x  = element_blank(),axis.ticks.x=element_blank(),axis.title.x=element_blank(),axis.text.y  = element_text(size = 7))+
  geom_tile()
#ggsave("images/employeeHeatMapSig.png",plot = last_plot(), type = png())
# EDA For Key Sign Fields on Attrition for Overall Model
ggkeySignPairs <- ggpairs(
  mapping = ggplot2::aes(color = emp_result$Attrition),
  emp_result[,c(cols.KeyFieldsBaseModel)], 
  diag=list(continuous="densityDiag", discrete="barDiag"), 
  axisLabels="show") + theme_minimal()
#ggsave("ggkeySignPairs.png",plot = last_plot(), type = png())
```

#### *Signficant Key Factor attributing to Attirtion*

Showing the significance, p-values, for each variable. This is the base information to understand the key contributors that affect attrition. This data will be used through our data science work so we can utilize the following variables for the models we build.

The following table outlines the top significant factors contributing to attrition:

```{r BaseGLM}
#TrainDataSet
glm_modeltrain <- glm(emp_train$Attrition~.,emp_train[,c(cols.CatGLM)], family = binomial) # glm train
model_Train = data.frame(coef(summary(glm_modeltrain))[,4]) # pvalue from glm train
names(model_Train) = "Logistic Regression on Training Set" # title 

# Table consolidated
GLM_dataset <-cbind(model_Train) # consolidated train, test and all data set
# Creating kable table for GLM dataset
GLM_dataset  %>%  kable() %>% kable_styling(bootstrap_options = "striped", full_width = F) %>% scroll_box(width = "600px", height = "450px")
```

#### *Basic Regrewssion for each Job Role*

Learning about any job role specific trends that may exist in the data set is key because it tells us which variables are significant by job.  This data can be used to identify what affects will contribute to attrition rate by job. Any value that is < 0.5 is significant

From the tables below, each job description will show the key attributes for attrition:

```{r DetailGLM}
# Glm for Job role - Human Resources
glm_model_JobRoleHR <- glm(emp_result$`JobRole_Human Resources`~.,emp_result[,c(col.NoJobRole)], family = binomial) # glm
JobRoleHR = data.frame(coef(summary(glm_model_JobRoleHR))[,4]) # pulling only pvalue from the glm
names(JobRoleHR) = "Human Resources" # creating title
# Glm for Job role - Manufactoring Director
glm_model_JobRoleManufactoring <- glm(emp_result$`JobRole_Manufacturing Director`~.,emp_result[,c(col.NoJobRole)], family = binomial)# glm
JobRoleManufactoring = data.frame(coef(summary(glm_model_JobRoleManufactoring))[,4])# pulling only pvalue from the glm
names(JobRoleManufactoring) = "Manufacturing Director" # creating title
# Glm for Job role - Research Scientist
glm_model_JobRoleResearch <- glm(emp_result$`JobRole_Research Scientist`~.,emp_result[,c(col.NoJobRole)], family = binomial)# glm
JobRoleResearch = data.frame(coef(summary(glm_model_JobRoleResearch))[,4])# pulling only pvalue from the glm
names(JobRoleResearch) = "Research Scientist" # creating title
# Glm for Job role - Lab Tech
glm_model_JobRoleLab <- glm(emp_result$`JobRole_Laboratory Technician`~.,emp_result[,c(col.NoJobRole)], family = binomial)# glm
JobRoleLab = data.frame(coef(summary(glm_model_JobRoleLab))[,4])# pulling only pvalue from the glm
names(JobRoleLab) = "Laboratory Technician" # creating title
# Glm for Job role - Research Director
glm_model_JobRoleResearchDirector <- glm(emp_result$`JobRole_Research Director`~.,emp_result[,c(col.NoJobRole)], family = binomial)# glm
JobRoleRD = data.frame(coef(summary(glm_model_JobRoleResearchDirector))[,4])# pulling only pvalue from the glm
names(JobRoleRD) = "Research Director" # creating title
# Glm for Job role - Sales Exec
glm_model_JobRoleSalesExec <- glm(emp_result$`JobRole_Sales Executive`~.,emp_result[,c(col.NoJobRole)], family = binomial)# glm
JobRoleSE = data.frame(coef(summary(glm_model_JobRoleSalesExec))[,4])# pulling only pvalue from the glm
names(JobRoleSE) = "Sales Executive" # creating title
# Glm for Job role - Sales Person
glm_model_JobRoleSalesPerson <- glm(emp_result$`JobRole_Sales Representative`~.,emp_result[,c(col.NoJobRole)], family = binomial)# glm
JobRoleSP = data.frame(coef(summary(glm_model_JobRoleSalesPerson))[,4])# pulling only pvalue from the glm
names(JobRoleSP) = "Sales Representative" # creating title
# Glm for Job role - Manager
glm_model_JobRoleManager <- glm(emp_result$JobRole_Manager~.,emp_result[,c(col.NoJobRole)], family = binomial)# glm
JobRoleManager = data.frame(coef(summary(glm_model_JobRoleManager))[,4])# pulling only pvalue from the glm
names(JobRoleManager) = "Manager" # creating title
# Glm for Job role - HealthCare
glm_model_JobRoleHealth <- glm(emp_result$`JobRole_Healthcare Representative`~.,emp_result[,c(col.NoJobRole)], family = binomial)# glm
JobRoleHealthR = data.frame(coef(summary(glm_model_JobRoleHealth))[,4])# pulling only pvalue from the glm
names(JobRoleHealthR) = "Healthcare Representative" # creating title
# Gener by Job Role
glm_model_Gender  <- glm(emp_train$Gender~.,emp_train[,c(cols.JobRoles)], family = binomial)# glm
Gender_Model = data.frame(coef(summary(glm_model_Gender))[,4])# pulling only pvalue from the glm
names(Gender_Model) = "Gender" # creating title
# Marital Status by Role
glm_model_Marital  <- glm(emp_train$MaritalStatus~.,emp_train[,c(cols.JobRoles)], family = binomial)# glm
Marital_Model = data.frame(coef(summary(glm_model_Marital))[,4])# pulling only pvalue from the glm
names(Marital_Model) = "Marital Status" # creating title
# Consolidated all the job role glm
Table.glm <-cbind(JobRoleHR, JobRoleManufactoring,JobRoleResearch,JobRoleLab,JobRoleRD,JobRoleSE,JobRoleManager)
# kable output for the consolidated glm
Table.glm  %>%  kable() %>% kable_styling(bootstrap_options = "striped", full_width = F) %>% scroll_box(width = "800px", height = "450px")
# kable output for the Gender by Job Role glm
Gender_Model  %>%  kable() %>% kable_styling(bootstrap_options = "striped", full_width = F) %>% scroll_box(width = "500px", height = "450px")
# kable output for the Martial Status Role glm
Marital_Model  %>%  kable() %>% kable_styling(bootstrap_options = "striped", full_width = F) %>% scroll_box(width = "500px", height = "450px")
```
#### *KNN Model*

Running the full KNN model using the training and test data set. The full KNN model came out to have a high accuracy rate of 84%, from there we decided to run the KNN model by job role. The glm showed us that each job has different variables of significance, so the KNN by job reflects different variables that pertains to that specific role.

As a key note, KNN works better with larger data sets than splitting them into job positions.

```{r KNN}
# KNN
set.seed(123)
#knn.train = train(Attrition~., data=emp_train[,c(cols.CatKNN)], method="knn", trControl=control, tuneGrid=grid1)
knn.train = train(Attrition~., data=emp_train[,c(cols.CatKNN)], method="knn")
knn.train
#Set K=18 sq of 1480
knn.test = knn(emp_train[,c(cols.CatKNN)][,-3], emp_test[,c(cols.CatKNN)][,-3], emp_train[,c(cols.CatKNN)][,3], k=18)
knnPrediction <-confusionMatrix(table(knn.test, emp_test$Attrition))
knnPrediction
fourfoldplot(knnPrediction$table)
```

#### *KNN Weighted*
Running the Weighted KNN model using the training and test data set. The Weighted KNN model came out to have a higher accuracy rate of 84.4 than the KNN which was 84%. Additional, the plot below shows the optimal K which is 30.

```{r KWeighted}
# K Weighted
set.seed(123)
#performs leave-one-out crossvalidation 
kknn.train = train.kknn(Attrition~., data=emp_train[,c(cols.CatKNN)], kmax=30, distance = 2)
#Predict Attribution
prediction <- predict(kknn.train, emp_test[,c(cols.CatKNN)][,-3])
#Show Confusion Matrix
kWeightedPrediction <- confusionMatrix(table(prediction, emp_test[,c(cols.CatKNN)][,3]))
kWeightedPrediction
fourfoldplot(kWeightedPrediction$table)
```
#### *Logistic_Regression*

The following model is logistic regression the test and training set for all the fields.  For logistic regression to work, the data was formatted to numeric data and setup with a prediction interval in which we converted a probability. In this model, we predicted at an 87% rate.

```{r Logistic_Regression}
# Logistic Regression (No Lasso) - Winning Model
#Base Model
glm_model <- glm(emp_train$Attrition~.,data = emp_train[,c(cols.CatGLM)], family = binomial)
summary(glm_model)
#predict probabilities on testset
#type="response" gives probabilities, type="class" gives class
glm_prob <- predict.glm(glm_model,emp_test[,-3],type="response")
#which classes do these probabilities refer to? What are 1 and 0?
contrasts(emp_test$Attrition)
#make predictions
##.first create vector to hold predictions (we know 0 refers to neg now)
dfTrain <- rep("No",nrow(emp_test))
dfTrain[glm_prob>.5] <- "Yes"
#confusion matrix
LogRegOnly <-confusionMatrix(table(pred=dfTrain,true=emp_test$Attrition))
LogRegOnly
fourfoldplot(LogRegOnly$table)
#Predict for Doctor Bivin - WHAMMMMooooooo
dfPreds = data.frame(emp_test$ID,dfTrain)
colnames(dfPreds) = c("ID","Prediction")
dfPreds
write.csv(dfPreds,file = "LabelPrediction.csv",row.names = FALSE)
```

##### Summary Explanation:
The following table outlines the four different models that were used in accuracy order.  As you can tell, logistic regression was the most accurate; however, we recommend using the logistic regression using lasso because it is more efficient. 

```{r MLSummaryModels}
# Prediciton Models
# Review Prediciton Models
LogRegOnly # Log Regression
knnPrediction # kNN 
kWeightedPrediction # K Weighted
# Create Prediction Summary Table
dt0 <- data.frame(cbind(t(LogRegOnly$overall),t(LogRegOnly$byClass)))
dt0$Type <- as.character("LogRegOnly")
dt1 <- data.frame(cbind(t(knnPrediction$overall),t(knnPrediction$byClass)))
dt1$Type <- as.character("kNN")
dt3 <- data.frame(cbind(t(kWeightedPrediction$overall),t(kWeightedPrediction$byClass)))
dt3$Type <- as.character("kWeighted")
SummaryPred <-rbind(dt0, dt1, dt3)
SummaryPred <- SummaryPred[order(-SummaryPred$Accuracy),]
SummaryPred <- SummaryPred[,c(19,1:18)]
SummaryT <- SummaryPred[,c(1,2,9, 10)]
SummaryT  %>%  kable() %>% kable_styling(bootstrap_options = "striped", full_width = F) %>% scroll_box(width = "100%", height = "200px")
```

##### Appendix
  - YouTube link: https://youtu.be/oCrYuL4jm-g
  - Github link: https://github.com/joshyi67/SMU-MSDS-Homework/tree/master/Case%20Study%202 